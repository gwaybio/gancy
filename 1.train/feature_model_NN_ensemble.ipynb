{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/gancy/merged_training.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, features):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    X = np.asarray(df[features], dtype='float')\n",
    "\n",
    "    # Get targets to one-hot encoding\n",
    "    f = pd.factorize(df['target'], sort=True)\n",
    "    y = np.zeros((f[0].shape[0], len(set(f[0]))))\n",
    "    y[np.arange(f[0].shape[0]), f[0].T] = 1\n",
    "    targets = f[1]\n",
    "    \n",
    "    return X, y, targets\n",
    "\n",
    "def load_validation_data(filename, features):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    X = np.asarray(df[features], dtype='float')\n",
    "        \n",
    "    X_codes = df['cell_code']\n",
    "    \n",
    "    return X, X_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_model_no_test(X_train, y_train, param, targets):   \n",
    "    # Hyper-parameters\n",
    "    input_units = X_train.shape[1]\n",
    "    output_units = len(targets)\n",
    "    \n",
    "    # Construct NN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=input_units, kernel_initializer='glorot_normal'))\n",
    "    model.add(normalization.BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(20, kernel_initializer='glorot_normal'))\n",
    "    model.add(normalization.BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(output_units, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Optimization method\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    # Fit training data\n",
    "    hist = model.fit(X_train, y_train, epochs=params['num_epochs'],\n",
    "                     batch_size=params['batch_size'], validation_split=params['percent_to_valid'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_NN_model(X_train, X_test, y_train, y_test, param, targets):   \n",
    "    # Hyper-parameters\n",
    "    input_units = X_train.shape[1]\n",
    "    output_units = len(targets)\n",
    "    \n",
    "    # Construct NN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=input_units, kernel_initializer='glorot_normal'))\n",
    "    model.add(normalization.BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(20, kernel_initializer='glorot_normal'))\n",
    "    model.add(normalization.BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(output_units, kernel_initializer='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Optimization method\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    # Fit training data\n",
    "    hist = model.fit(X_train, y_train, epochs=params['num_epochs'],\n",
    "                     batch_size=params['batch_size'], validation_split=params['percent_to_valid'])\n",
    "\n",
    "    # Evaluate model\n",
    "    score = model.evaluate(X_test, y_test, batch_size=params['batch_size'])\n",
    "    print('\\nTest %s: %.2f' % (model.metrics_names[0], score[0]))\n",
    "    print('Test %s: %.2f%%' % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test, batch_size=params['batch_size'])\n",
    "    y_pred = [targets[i] for i in np.argmax(y_pred, axis=1)]\n",
    "    y_true = [targets[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "    # Print prediction performance on F1-score\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')  \n",
    "    print('F1-Score: %.2f' % f1)\n",
    "    \n",
    "    unpredicted_classes = list(set(y_true) - set(y_pred))\n",
    "    if len(unpredicted_classes):\n",
    "        print('THESE CLASSES ARE NOT PREDICTED - SCORING WILL FAIL!')\n",
    "        print(unpredicted_classes)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    #cm = confusion_matrix(y_true, y_pred, targets)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    #plt.imshow(cm, interpolation='nearest', cmap=plt.cm.viridis)\n",
    "    #tick_marks = np.arange(len(targets))\n",
    "    #plt.xticks(tick_marks, targets, rotation=45, ha='right')\n",
    "    #plt.yticks(tick_marks, targets)\n",
    "    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    #    plt.text(j, i, cm[i, j], horizontalalignment='center', color='black')\n",
    "\n",
    "    #plt.ylabel('True label')\n",
    "    #plt.xlabel('Predicted label')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(validation_filename, features, output_prefix, targets, runn):\n",
    "    # Load validation data\n",
    "    X_valid, valid_cell_codes = load_validation_data(validation_filename, features)\n",
    "\n",
    "    # Predict target probabilities\n",
    "    y_valid_prob = model.predict(X_valid, batch_size=params['batch_size'])\n",
    "    df_valid_prob = pd.DataFrame(columns=target_names, data=y_valid_prob)\n",
    "    df_valid_prob.insert(0, 'cell_code', valid_cell_codes)\n",
    "\n",
    "    # Predictions on maximum probabilities\n",
    "    df_valid_pred = pd.DataFrame(columns = ['cell_code', 'prediction'])\n",
    "    df_valid_pred['cell_code'] = df_valid_prob['cell_code']\n",
    "    df_valid_pred['prediction'] = [targets[i] for i in np.argmax(y_valid_prob, axis=1)]\n",
    "\n",
    "    # Save predictions\n",
    "    df_valid_prob.to_csv('%s_probabilities_run-%d.csv' % (output_prefix, runn), index=False)\n",
    "    #df_valid_pred.to_csv('%s_run-%d.csv' % (output_prefix, runn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input options\n",
    "\n",
    "training_filename = '../raw_data/gancy/merged_training.csv'\n",
    "testing_filename = 'feature_data/testing_feature_select_robust.csv'\n",
    "validation_filename = '../raw_data/gancy/merged_validation.csv'\n",
    "\n",
    "features = ['actin.s.area', 'actin.s.radius.mean',\n",
    "       'actin.s.radius.sd', 'actin.s.radius.min', 'actin.b.sd',\n",
    "       'actin.b.mad', 'actin.b.q005', 'actin.b.q01', 'actin.b.q05',\n",
    "       'actin.m.cx', 'actin.m.eccentricity', 'actin.m.theta',\n",
    "       'DNA.s.area', 'DNA.s.radius.sd', 'DNA.s.radius.min', 'DNA.b.sd',\n",
    "       'DNA.b.mad', 'DNA.b.q005', 'DNA.b.q05', 'DNA.m.cy',\n",
    "       'DNA.m.majoraxis', 'DNA.m.eccentricity', 'DNA.m.theta',\n",
    "       'dist.10.nn', 'dist.30.nn', 'nuclear.displacement', 'FC2_14',\n",
    "       'FC2_46', 'FC2_78', 'FC2_86', 'FC2_91', 'FC2_125', 'FC2_142',\n",
    "       'FC2_157', 'FC2_257', 'FC2_260', 'FC2_262', 'FC2_283', 'FC2_308',\n",
    "       'FC2_335', 'FC2_352', 'FC2_398', 'FC2_401', 'FC2_473', 'FC2_477']\n",
    "\n",
    "params = {'percent_to_valid': 0.2,\n",
    "          'percent_to_test': 0.1,\n",
    "          'num_epochs': 50,\n",
    "          'batch_size': 256}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    X_train, y_train, target_names = load_data(training_filename, features)\n",
    "    #X_test, y_test, target_names = load_data(testing_filename, features)\n",
    "    #model = train_NN_model(X_train, X_test, y_train, y_test, params, target_names)\n",
    "    model = train_NN_model_no_test(X_train, y_train, params, target_names)\n",
    "    output_prefix = 'combined_data_predictions_NN/combined_features'\n",
    "    save_predictions(validation_filename, features, output_prefix, target_names, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
